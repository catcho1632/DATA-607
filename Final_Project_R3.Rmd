---
title: 'Final Project: Job Recommender'
author: by Team Shiny (William Aiken, Bikram Barua, Catherine Cho, Eric Lehmphul, Nnaemeka Newman
  Okereafor)
date: "11/27/2021"
output: rmdformats::readthedown
---

### Introduction: This project will build a very basic job recommendation system using a content-based approach. 

#### Purpose 1: In this project, we present a recommender system designed for the job seeker in Data Science. The proposed recommender system aims at leveraging the jobs and companies that are important for a target candidate. To meet this objective, job descriptions and candidate resumes are examined along with other user inputs. The recommendation approach is modeled on content-based analysis using natural language processing. The dataset consisted of scraped job postings from [Glassdoor](http://www.glassdoor.com) and resumes from [Post Resumes Free](https://www.postjobfree.com/resume/adktqz/senior-data-scientist-brooklyn-ny) 

#### Purpose 2: The second purpose is to provide the job seeker information about their qualifications in relation to Data Scientists in industry. In 2018, Kaggle conducted a survey to gather information on the state of data science and machine learning around the world. A total of 23,859 responses were collected and was compiled for analysis and the raw data can be found via https://www.kaggle.com/kaggle/kaggle-survey-2018?select=SurveySchema.csv. The candidate's resume is compared against this dataset to understand what compensation could be expected based on the type and number of Data Science related skills.

##### Approach: We have applied the following process to organize our workflow:

1) Business Understanding - TBDs
2) Data Understanding - ...
3) Data Preparation -  ...
4) Modeling - ...
5) Evaluation -  ...
6) Integration and Deployment - ...



############################################################################
# Instructions
This project is a proof-of-concept(POC) with certain assumptions on the data. For this implementation, Purpose 1 will be demonstrated via a markdown file below to show step by step how the text data is processed. Purpose 2 will be displayed in the Shiny App to allow the user to easily manipulate and filter settings to gain more insight of the job market today. 

#Load the libraries
```{r}
library(tidyverse)
library(tidytext)
library(httr)
library(rvest)
library(stringr)
library(readr)
library(tm)
library(slam)
library(dplyr)
library(tidytext)
library(tidyr)
library(dplyr)
library(textstem)
library(lsa)
library(data.table)
library(VennDiagram)
library(wordcloud)
library(RColorBrewer)
library(ggplot2)
library(wordcloud)
library(httr)
```
# Recommender System: Cosine Similarity

## Load the data: The scraped job postings are stored in a dataframe and read in below. A sample resume is scraped and the raw text is stored in variable, "resume". 
```{r warning=FALSE, message=FALSE}
library(readr)
urlfile<-"https://raw.githubusercontent.com/baruab/Team2_Project_3_607/main/job_posting.csv"
jobs<-read_csv(url(urlfile))

#url_res<-"https://www.postjobfree.com/resume/adktqz/senior-data-scientist-brooklyn-ny"
#url_res<-"https://www.postjobfree.com/resume/adk07o/data-science-new-york-ny"
#url_res<-"https://www.postjobfree.com/resume/adol8d/data-scientist-new-york-ny"
#url_res<-"https://www.postjobfree.com/resume/adost3/data-scientist-new-york-ny"
url_res<-"https://www.postjobfree.com/resume/adonl3/data-scientist-charlotte-nc"
#url_res<-"https://www.postjobfree.com/resume/ado61j/data-scientist-arlington-va"
#url_res<-"https://www.postjobfree.com/resume/adol8d/data-scientist-new-york-ny"
web<- read_html(url_res)
resume<-web %>%html_nodes(".normalText")%>%html_text()

head(jobs)
head(resume)

```
## A total of 2527 Data Science related Job postings are available for the candidate to consider. However, 300 are evaluated here to save on computation time. The job descriptions are stored as raw text in a new dataframe, "des_all". 
```{r warning=FALSE, message=FALSE}
##Multiple Job postings at once (Corpus)
#One row of posting
postings<-300
des_all<-subset(jobs,select=c(3))
#des_all<-data.frame(jobs$job_description)
des_all<-des_all[1:postings,]
head(des_all)
```

## The variable "resume", which contains the resume  text file is stored as the last row of the des_all dataframe (after all the job postings are listed in preceding rows). In preparation for NLP, the text is processed by:
1) Using regular expressions, unnecesssary symbols and notations are removed. 
2) Stop words are removed. 
3) All letters are brought to lower case. 
4) Punctuations are removed. 
5) Each string is lemmatized to bring to its basic form. 


```{r warning=FALSE, message=FALSE}
#adding resume text as doc_id last
des_all<-rbind(des_all,resume)

des_all$job_description<-des_all$job_description%>%
  str_replace_all(pattern="\n",replacement=" ")%>%
  str_replace_all(pattern="www+|com|@\\S+|#\\S+|http|\\*|\\s[A-Z]\\s|\\s[a-z]\\s|\\d|ï¿½+",replacement=" ")
des_all$job_description<-tolower(des_all$job_description)
des_all$job_description<-removeNumbers(des_all$job_description)
des_all$job_description<-removePunctuation(des_all$job_description)
#des_all$job_description<-stripWhitespace(des_all$job_description)
des_all$job_description<-removeWords(des_all$job_description,stopwords("en"))
des_all$job_description<-sapply(des_all$job_description,lemmatize_strings)

head(des_all)
```

## The job descriptions are stored in a Volatile Corpus and the words are tokenized into a matrix. The term frequency per job description is recorded in the matrix and the terms are weighted using term frequency-inverse document frequency (tfidf). The tfidf offsets the number of times a term appears in a  document by the number of documents in the corpous that contain the word. This ensures that terms that simply appear more times than others are not incorrectly considered to be significant since a term can simply appear more frequently if a document has more text.   
```{r warning=FALSE, message=FALSE}
des_all_df<-data.frame(
  doc_id=1:(postings+1),
  text=des_all$job_description
)

Corpus=VCorpus(DataframeSource(des_all_df))

tf<-DocumentTermMatrix(Corpus,control=list(weighting=weightTf))
tfidf<-DocumentTermMatrix(Corpus,control=list(weighting=weightTfIdf))

inspect(tf)
inspect(tfidf)
```

## Build Recommender: The similarity between job descriptions of each job posting to the candidates resume is assessed using cosine similarity. Mathematically, the cosine similarity  measures the cosine of the angle between two vectors projected. The closer the output is to 1, the more similar the objects are. The lsa package is used to calculate the cosine matrix. Since the resume text is stored in the last row of the matrix, the last row of the cosine similarity output will compare the resume to all the job postings. 
```{r warning=FALSE, message=FALSE}
#test cosine
tfidf_a<-as.matrix(tfidf)
tfidf_a<-transpose(data.frame(tfidf_a))
tfidf_a<-as.matrix(tfidf_a)

cos_df<-data.frame(cosine(tfidf_a))
resume_similarity<-cos_df[(postings+1),]
head(resume_similarity)

```

## The job posting dataframe is re-arranged based on the cosine similarity output. As the row number increases, the lower the similarity between the resume and job posting. The new dataframe is called "rec_df".  
```{r warning=FALSE, message=FALSE}
list<-names(resume_similarity)<-NULL
list<-unlist(c(resume_similarity))
order<-order(list,decreasing=TRUE)
order<-order[-c(1)]
doc_ID<-data.frame(order)

rec_df<-doc_ID
colnames(rec_df)<-c("doc_ID")

rec_df<-rec_df%>%
  mutate(job_title=jobs[order,2])%>%
  mutate(min_salary=jobs[order,4])%>%
  mutate(max_salary=jobs[order,5])%>%
  mutate(city=jobs[order,6])%>%
  mutate(state=jobs[order,7])%>%
  mutate(company_name=jobs[order,8])%>%
  mutate(company_industry=jobs[order,9])%>%
  mutate(company_rating=jobs[order,10])%>%
  mutate(bachelors=jobs[order,11])%>%
  mutate(masters=jobs[order,12])%>%
  mutate(PHD=jobs[order,13])
rec_df<-unnest(rec_df)
head(rec_df)
```

## The following section writes a function rec(), which will return a word cloud and venn diagram, and the job postings associated with them, showing what attributes were similar between these top ranked recommendations. The first 3 recommendations are shown. 
```{r warning=FALSE, message=FALSE}

#understanding the terms that are most relevant
rec<-function(ranking){
row_num<-ranking+1
z<-data.frame(as.matrix(tfidf)) 
compare1<-rbind(z[ranking,],z[nrow(z),]) 
comp1<-compare1%>%
  mutate(row_n=1:n())%>%
  select_if(function(x) any(x!=0 & .$row_n!=0))
comp1_t<-transpose(comp1)
colnames(comp1_t)<-c("Resume","Job")
comp1_t$terms<-colnames(comp1)
comp1_matches<-comp1_t[comp1_t$Resume!=0 & comp1_t$Job!=0,]
rownames(comp1_matches)<-NULL
comp1_matches<-comp1_matches[-c(nrow(comp1_matches)),]
comp1_matches_n<-nrow(comp1_matches)

Job1_diff<-comp1_t[comp1_t$Resume==0 & comp1_t$Job!=0,]
Job1_diff_n<-nrow(Job1_diff)
Resume1_diff<-comp1_t[comp1_t$Resume!=0 & comp1_t$Job==0,]
Resume1_diff_n<-nrow(Resume1_diff)
#Venn Diagram of common and different words between resume and job posting. 
grid.newpage()
draw.pairwise.venn(Resume1_diff_n+comp1_matches_n, Job1_diff_n+comp1_matches_n, comp1_matches_n, category = c("Terms in your resume", "Terms in Job Posting"), lty = rep("blank", 
    2), fill = c("light blue", "pink"), alpha = rep(0.5, 2), cat.pos = c(0, 
    0), cat.dist = rep(0.025, 2), scaled = FALSE)
#Word Cloud for top recommended job
comp1_matches_adjust<-comp1_matches%>%
  mutate(comp1_matches$Job*1000)
colnames(comp1_matches_adjust)<-c("Resume","Job","terms","adjust")
set.seed(1234)
wordcloud(words = comp1_matches_adjust$terms, freq = comp1_matches_adjust$adjust, min.freq = 1,
          max.words=200, random.order=FALSE, rot.per=0.35, 
          colors=brewer.pal(8, "Dark2"))
return(rec_df[ranking,2:9])
}

```

```{r warning=FALSE, message=FALSE}

rec(1)
rec(2)
rec(3)
```


# Conclusion and Future Improvements: The recommender built in this system is understood to be a very rudimentary approach that demonstrates how cosine similarity can be used to assess the similarity between documents. Since this approach requires the processing of all the job descriptions in its entirety, this can be computationally expensive. A future improvement could be to explore and implement text classification, which is a machine learning technique that assigns a set of categories to text. Much of the computation of classifying text can be performed off line which would drastically shorten solve time. This approach however shows that some of the significant terms captured are data science related and potentially characteristics an employer may be interested in when searching for a candidate.   


# Create the Shiny UI


## Authenticate the user

## Gather Inputs


## Present visualizations




# Conclusion

#Future Improvement

# Shiny Apps Links

# Q & A

